{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bd901d0a-bebf-4cdb-99d4-e623d03f3911",
   "metadata": {},
   "source": [
    "# Prompt Error Identification\n",
    "**Purpose:** Score and identify errors in individual prompts against gold standard\n",
    "\n",
    "---\n",
    "**Copyright (c) 2025 Michael Powers**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36133366-2570-473e-9914-66b7babb163a",
   "metadata": {},
   "source": [
    "# run subquestion results through gemini for scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "d8f43b6b-028f-4e05-b577-23d4f2555117",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "import json\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import google.generativeai as genai\n",
    "import time\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "5ec9082b-2082-4664-bfd1-2b857d9dabc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "gemini_model = 'gemini-2.5-flash-lite-preview-06-17'\n",
    "api_key=\"YOUR_API_KEY\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "0a364b6a-e2ae-428a-a81a-62ea0c01ebc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_gemini_json(prompt, use_json=True, model='models/gemini-2.0-flash-lite'):\n",
    "    import os\n",
    "    import google.generativeai as genai\n",
    "    genai.configure(api_key=api_key)\n",
    "    model = genai.GenerativeModel(model)\n",
    "    if use_json:\n",
    "        generation_config = genai.GenerationConfig(response_mime_type=\"application/json\")\n",
    "        response = model.generate_content(prompt, generation_config=generation_config)\n",
    "    else:\n",
    "        response = model.generate_content(prompt)\n",
    "    return response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "7c911b01-349b-4cd2-a7ba-03498431da50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_response(response):\n",
    "    response = response.replace(\"<think>\", \"\").replace(\"</think>\", \"\")\n",
    "    response = response.strip()\n",
    "    if response.startswith(\"```json\") and response.endswith(\"```\"):\n",
    "        response = response[len(\"```json\"): -len(\"```\")].strip()\n",
    "    elif response.startswith(\"```\") and response.endswith(\"```\"):\n",
    "        response = response[len(\"```\"): -len(\"```\")].strip()\n",
    "    if response.lower().startswith('sql'):\n",
    "        response = response[3:].strip()\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "e658c54c-129d-402d-88aa-296f8e6f6da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prompt(sub_prompt, generated_response, \n",
    "               original_user_query, tables_to_be_used,full_gold_star_sql, sub_prompt_purpose):\n",
    "\n",
    "    json_format = \"\"\"\n",
    "    ```json\n",
    "{\n",
    "  \"did_error_occur\": true/false,\n",
    "  \"error_category\": \"string\",\n",
    "  \"why_error_occurred\": \"string\",\n",
    "  \"suggestions_for_improvement\": \"string\",\n",
    "  \"score\": integer (1-5)\n",
    "}\n",
    "```\n",
    "    \"\"\"\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "You are an expert AI assistant tasked with evaluating the performance of individual \"sub-prompts\" within a RAG-to-SQL system. Your goal is to assess how accurately and effectively a sub-prompt's generated output contributes to building the correct full SQL query for a given user question.\n",
    "\n",
    "**Crucially, your evaluation MUST be strictly aligned with the defined purpose of the 'sub_prompt' provided in `<SUB_PROMPT_PURPOSE>`. Do not infer or assume a different purpose for the sub-prompt than what is explicitly stated there.**\n",
    "\n",
    "You will be provided with the following information:\n",
    "- The sub-prompt itself (`sub_prompt`).\n",
    "- The response generated by the sub-prompt (`generated_response`).\n",
    "- The original user query (`original_user_query`).\n",
    "- A list of table names available in the database (`tables_to_be_used`).\n",
    "- The complete, correct SQL query that should be generated by the entire RAG-to-SQL process (`full_gold_star_sql`).\n",
    "- The defined purpose of the 'sub_prompt' ('sub_prompt_purpose')\n",
    "\n",
    "Your evaluation should consider the specific purpose of the sub-prompt based on`sub_prompt_purpose` (e.g., identifying tables, grouping, calculations, filtering, joins). You must determine if the `generated_response` from the sub-prompt provides the correct and necessary information to lead to the `full_gold_star_sql` in its specific domain, *as defined by its stated purpose*.\n",
    "\n",
    "**Input:**\n",
    "<SUB_PROMPT>\n",
    "{sub_prompt}\n",
    "</SUB_PROMPT>\n",
    "\n",
    "<GENERATED_RESPONSE>\n",
    "{generated_response}\n",
    "</GENERATED_RESPONSE>\n",
    "\n",
    "<ORIGINAL_USER_QUERY>\n",
    "{original_user_query}\n",
    "</ORIGINAL_USER_QUERY>\n",
    "\n",
    "<TABLES_TO_BE_USED>\n",
    "{tables_to_be_used}\n",
    "</TABLES_TO_BE_USED>\n",
    "\n",
    "<FULL_GOLD_STAR_SQL>\n",
    "{full_gold_star_sql}\n",
    "</FULL_GOLD_STAR_SQL>\n",
    "\n",
    "<SUB_PROMPT_PURPOSE>\n",
    "{sub_prompt_purpose}\n",
    "</SUB_PROMPT_PURPOSE>\n",
    "\n",
    "**Evaluation Criteria and Output Format:**\n",
    "\n",
    "Based on the above inputs, analyze the `generated_response` in the context of the `sub_prompt`'s explicitly stated purpose (`<SUB_PROMPT_PURPOSE>`) and the `full_gold_star_sql`. Provide your assessment as a JSON object with the following fields:\n",
    "\n",
    "1.  **`did_error_occur`**: A boolean (`true` if an error or significant imperfection occurred, `false` if the `generated_response` was perfectly correct and useful for its sub-prompt's goal).\n",
    "2.  **`error_category`**: A string representing the most relevant error type if `did_error_occur` is `true`. If `did_error_occur` is `false`, set this to \"No Error\". Choose from the following categories:\n",
    "    * `Schema Misidentification`: The sub-prompt incorrectly identified or missed relevant tables or columns.\n",
    "    * `Incorrect Aggregation/Calculation`: The sub-prompt suggested wrong aggregations (e.g., SUM instead of COUNT), wrong columns for aggregation, or missed required calculations.\n",
    "    * `Incorrect Filtering`: The sub-prompt suggested wrong filtering conditions, wrong columns for filters, or missed required filters (WHERE/HAVING clauses).\n",
    "    * `Incorrect Join Logic`: The sub-prompt suggested wrong join types (e.g., LEFT instead of INNER), wrong join columns, or missed required joins.\n",
    "    * `Incorrect Grouping`: The sub-prompt suggested wrong grouping columns or missed required GROUP BY clauses.\n",
    "    * `Misinterpretation of User Intent`: The sub-prompt's understanding of the user query was fundamentally flawed, leading to an irrelevant or incorrect `generated_response`.\n",
    "    * `Irrelevant/Hallucinated Information`: The sub-prompt included information not requested or entirely made up.\n",
    "    * `Syntax/Format Error`: The `generated_response` itself was not in the expected format (e.g., malformed JSON, unparseable text).\n",
    "    * `Partial Correctness`: The `generated_response` was partially correct but had significant omissions or minor inaccuracies that did not fit the more specific categories.\n",
    "    * `False Negative`: The `generated_response` claimed the sub-prompt was not needed but according to the `full_gold_star_sql` it was needed. (e.g. claimed No Filtering needed when Filtering was needed)\n",
    "    * `Other Error`: An error occurred that does not fit the above categories.\n",
    "    * `No Error`: The `generated_response` was fully correct and aligned with the `full_gold_star_sql`'s requirements for its specific sub-task.\n",
    "3.  **`why_error_occurred`**: A detailed string explaining *why* the error occurred, referring to specific parts of the `generated_response`, `original_user_query`, and `full_gold_star_sql`. **Crucially, if the `error_category` is 'Misinterpretation of Sub-Prompt Purpose', explain how the `generated_response` deviated from the `<SUB_PROMPT_PURPOSE>`**. If no error, state \"N/A\".\n",
    "4.  **`suggestions_for_improvement`**: A string providing actionable suggestions on how to modify or improve the `sub_prompt` itself to prevent this error in the future. Be specific. If no error, state \"N/A\".\n",
    "5.  **`score`**: An integer from 1 to 5, reflecting the performance of the `generated_response` in achieving its specific sub-prompt goal, in the context of contributing to the `full_gold_star_sql`.\n",
    "    * **5 (Excellent):** The `generated_response` is perfectly accurate and complete for its intended purpose, directly and correctly contributing to the `full_gold_star_sql`. No missing or incorrect information.\n",
    "    * **4 (Good):** The `generated_response` is mostly accurate and helpful, but might have minor omissions or slightly irrelevant information that doesn't hinder the overall process significantly towards the `full_gold_star_sql`.\n",
    "    * **3 (Acceptable):** The `generated_response` contains some correct information but also significant omissions or inaccuracies that require substantial correction or might lead to errors down the line. It's partially helpful for reaching the `full_gold_star_sql`.\n",
    "    * **2 (Poor):** The `generated_response` is largely incorrect, misleading, or missing critical information. It actively steers the process in the wrong direction or provides minimal value.\n",
    "    * **1 (Failed):** The `generated_response` is completely wrong, irrelevant, unusable, or actively harms the process.\n",
    "\n",
    "**JSON Output:**\n",
    "{json_format}\n",
    "\n",
    "\n",
    "    \n",
    "    \"\"\"\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "21240f27-e11e-42e2-b9ab-46c9c6f4e6c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_string(filename):\n",
    "    try:\n",
    "        # First, try to open with the standard UTF-8\n",
    "        with open(filename, 'r', encoding='utf-8') as f:\n",
    "            return f.read()\n",
    "    except UnicodeDecodeError:\n",
    "        # If UTF-8 fails, try a different encoding like latin-1\n",
    "        print(f\"Warning: '{filename}' is not UTF-8. Trying latin-1...\")\n",
    "        with open(filename, 'r', encoding='latin-1') as f:\n",
    "            return f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "85b91a83-a6d5-420f-992a-ea985911fb53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_value_by_id(jsonl_file, instance_id, target_key):\n",
    "    with open(jsonl_file, 'r') as f:\n",
    "        for i, line in enumerate(f):\n",
    "            try:\n",
    "                entry = json.loads(line.strip())\n",
    "                if entry.get(\"instance_id\") == instance_id:\n",
    "                    return entry.get(target_key)\n",
    "            except Exception as e:\n",
    "                print(f\"ERROR: line {i}: {e}\")\n",
    "    return None\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "7b4cbb3d-b67e-4ce9-aac7-b92f38bd0899",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_subprompts(test_directory=\"./results/\",\n",
    "                                gold_directory=\"./gold/\",\n",
    "                                output_filename=\"./results.csv\",\n",
    "                                prompt_to_evaluate = \"tables\",\n",
    "                                gold_tables_file = \"gold.json\",\n",
    "                                orig_prompt = \"hi\",\n",
    "                                original_questions_file = \"questions.jsonl\",\n",
    "                                prompt_purpose=\"win\",\n",
    "                                model=gemini_model,\n",
    "                                rpm_limit=15):\n",
    "\n",
    "    from datetime import datetime\n",
    "    \n",
    "    RPM_LIMIT = rpm_limit\n",
    "    MAX_RETRIES = 5\n",
    "    BASE_SLEEP_TIME = 8.5\n",
    "    df_rows = []\n",
    "    review_rows = []\n",
    "\n",
    "    num_responses = 0\n",
    "\n",
    "    print(f\"Starting generation.\")\n",
    "    #Loop through all files in test_directory\n",
    "    for item in os.listdir(test_directory):\n",
    "        item_path = os.path.join(test_directory, item)\n",
    "        if os.path.isfile(item_path) and item.endswith('.json'):\n",
    "            test_filename = item_path\n",
    "            try: \n",
    "                entry = json.loads(read_string(test_filename))\n",
    "                test_id = entry.get(\"ID\")\n",
    "                gold_filename = os.path.join(gold_directory, f\"{test_id}.sql\")\n",
    "                  \n",
    "                #gold for full sql\n",
    "                gold_contents = read_string(gold_filename)\n",
    "                #response to evaluate\n",
    "                to_evaluate = entry.get(prompt_to_evaluate)\n",
    "                #gold schema info\n",
    "                gold_schema = get_value_by_id(gold_tables_file, test_id, \"gold_tables\")\n",
    "                #original query info\n",
    "                orig_query = get_value_by_id(original_questions_file, test_id, \"question\")\n",
    "                \n",
    "\n",
    "                prompt = get_prompt(orig_prompt, to_evaluate, orig_query, gold_contents, gold_schema, prompt_purpose)\n",
    "              \n",
    "            \n",
    "                # Counter for API calls made within the current minute\n",
    "                requests_in_minute = 0\n",
    "                start_time_minute = time.time()\n",
    "                retries = 0\n",
    "        \n",
    "                while retries < MAX_RETRIES:\n",
    "                # Check RPM limit\n",
    "                    current_time = time.time()\n",
    "                    if current_time - start_time_minute >= 60:\n",
    "                        requests_in_minute = 0\n",
    "                        start_time_minute = current_time\n",
    "\n",
    "                    if requests_in_minute >= RPM_LIMIT:\n",
    "                        wait_time = 60 - (current_time - start_time_minute)\n",
    "                        print(f\"Rate limit hit. Waiting for {wait_time:.2f} seconds...\")\n",
    "                        time.sleep(wait_time + 1)\n",
    "                        requests_in_minute = 0\n",
    "                        start_time_minute = time.time()\n",
    "\n",
    "                    try: # LLM CALL\n",
    "                        print(f'Making LLM Call {num_responses+1}')\n",
    "                        response = ask_gemini_json(prompt, use_json=True, model=model)\n",
    "                        requests_in_minute += 1\n",
    "                \n",
    "                    # Clean the response string\n",
    "                        response = clean_response(response)\n",
    "                        if num_responses < 1:\n",
    "                            print(f\"FIRST RESPONSE:\\n{response}\")\n",
    "\n",
    "                        try:\n",
    "                            num_responses += 1\n",
    "                            data = json.loads(response)\n",
    "                            did_error_occur = data.get(\"did_error_occur\")\n",
    "                            df_rows.append({\n",
    "                                \"ID\" : test_id,\n",
    "                                \"contains_error\": data.get(\"did_error_occur\"),\n",
    "                                \"error_category\" : data.get(\"error_category\"),\n",
    "                                \"why_error_occurred\": data.get(\"why_error_occurred\"),\n",
    "                                \"suggestions\" : data.get(\"suggestions_for_improvement\"),\n",
    "                                \"score\": data.get(\"score\")\n",
    "                            })\n",
    "\n",
    "                            review_rows.append({\n",
    "                                \"ID\": test_id,\n",
    "                                \"score\": data.get(\"score\"),\n",
    "                                \"error_category\": data.get(\"error_category\"),\n",
    "                                \"model_response\": to_evaluate,\n",
    "                                \"reason\" : data.get(\"why_error_occurred\"),\n",
    "                            })\n",
    "                           \n",
    "                        except Exception as e:\n",
    "                            print(f'Error decoding json response: {e}')\n",
    "                            print(\"-------DATA NO GOOD\")\n",
    "                        \n",
    "                        break # Success, break out of retry loop\n",
    "                    except Exception as e:\n",
    "                        retries += 1\n",
    "                        sleep_duration = BASE_SLEEP_TIME * (2 ** (retries - 1)) + random.uniform(0, 1)\n",
    "                        print(f\"API Error : {e}. Retrying in {sleep_duration:.2f}s... (Attempt {retries}/{MAX_RETRIES})\")\n",
    "                        time.sleep(sleep_duration)\n",
    "\n",
    "                    if retries == MAX_RETRIES:\n",
    "                        print(f\"Failed to process review from input line {i} after {MAX_RETRIES} retries. Skipping.\")\n",
    "                        print(\"-------DATA NO GOOD\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error decoding test data: {e}\")\n",
    "                return\n",
    "            \n",
    "    df = pd.DataFrame(df_rows)\n",
    "    print(df)\n",
    "    df.to_csv(output_filename, index=False)\n",
    "\n",
    "    review_df = pd.DataFrame(review_rows)\n",
    "    review_df.to_csv(f\"ReviewResults_{datetime.now()}.csv\", index=False)\n",
    "    \n",
    "    return df\n",
    "    print(\"------DONE-----\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "242f135b-6906-41a0-8d11-e335c1fbfa38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " ---- START TABLES ------------\n",
      "Starting generation.\n",
      "Making LLM Call 1\n",
      "FIRST RESPONSE:\n",
      "{\n",
      "  \"did_error_occur\": false,\n",
      "  \"error_category\": \"No Error\",\n",
      "  \"why_error_occurred\": \"N/A\",\n",
      "  \"suggestions_for_improvement\": \"N/A\",\n",
      "  \"score\": 5\n",
      "}\n",
      "Making LLM Call 2\n",
      "Making LLM Call 3\n",
      "Making LLM Call 4\n",
      "Making LLM Call 5\n",
      "Making LLM Call 6\n",
      "Making LLM Call 7\n",
      "Making LLM Call 8\n",
      "Making LLM Call 9\n",
      "Making LLM Call 10\n",
      "Making LLM Call 11\n",
      "Making LLM Call 12\n",
      "Making LLM Call 13\n",
      "Making LLM Call 14\n",
      "Making LLM Call 15\n",
      "Making LLM Call 16\n",
      "Making LLM Call 17\n",
      "Making LLM Call 18\n",
      "Making LLM Call 19\n",
      "Making LLM Call 20\n",
      "Making LLM Call 21\n",
      "Making LLM Call 22\n",
      "Making LLM Call 23\n",
      "API Error : 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [violations {\n",
      "}\n",
      ", links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 3\n",
      "}\n",
      "]. Retrying in 8.59s... (Attempt 1/5)\n",
      "Making LLM Call 23\n",
      "API Error : 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [violations {\n",
      "}\n",
      ", links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 54\n",
      "}\n",
      "]. Retrying in 17.91s... (Attempt 2/5)\n",
      "Making LLM Call 23\n",
      "Making LLM Call 24\n",
      "API Error : 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [violations {\n",
      "}\n",
      ", links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 35\n",
      "}\n",
      "]. Retrying in 9.18s... (Attempt 1/5)\n",
      "Making LLM Call 24\n",
      "API Error : 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits. [violations {\n",
      "}\n",
      ", links {\n",
      "  description: \"Learn more about Gemini API quotas\"\n",
      "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
      "}\n",
      ", retry_delay {\n",
      "  seconds: 25\n",
      "}\n",
      "]. Retrying in 17.92s... (Attempt 2/5)\n",
      "Making LLM Call 24\n",
      "          ID  contains_error            error_category  \\\n",
      "0   local038           False                  No Error   \n",
      "1   local309           False                  No Error   \n",
      "2   local301           False                  No Error   \n",
      "3   local017            True  Schema Misidentification   \n",
      "4   local022            True  Schema Misidentification   \n",
      "5   local199            True  Schema Misidentification   \n",
      "6   local163           False                  No Error   \n",
      "7   local008           False                  No Error   \n",
      "8   local131           False                  No Error   \n",
      "9   local099            True  Schema Misidentification   \n",
      "10  local219           False                  No Error   \n",
      "11  local197           False                  No Error   \n",
      "12  local065            True  Schema Misidentification   \n",
      "13  local004           False                  No Error   \n",
      "14  local075            True  Schema Misidentification   \n",
      "15  local019           False                  No Error   \n",
      "16  local023           False                  No Error   \n",
      "17  local210            True  Schema Misidentification   \n",
      "18  local078           False                  No Error   \n",
      "19  local003            True  Schema Misidentification   \n",
      "20  local066            True  Schema Misidentification   \n",
      "21  local058           False                  No Error   \n",
      "22  local029           False                  No Error   \n",
      "23  local039            True  Schema Misidentification   \n",
      "\n",
      "                                   why_error_occurred  \\\n",
      "0                                                 N/A   \n",
      "1                                                 N/A   \n",
      "2                                                 N/A   \n",
      "3   The sub-prompt was tasked with identifying rel...   \n",
      "4   The sub-prompt's purpose was to identify table...   \n",
      "5   The generated response includes the 'inventory...   \n",
      "6                                                 N/A   \n",
      "7                                                 N/A   \n",
      "8                                                 N/A   \n",
      "9   The sub-prompt's purpose is to identify tables...   \n",
      "10                                                N/A   \n",
      "11                                                N/A   \n",
      "12  The generated response includes tables and col...   \n",
      "13                                                N/A   \n",
      "14  The generated response incorrectly identified ...   \n",
      "15                                                N/A   \n",
      "16                                                N/A   \n",
      "17  The sub-prompt's purpose was to identify all t...   \n",
      "18                                                N/A   \n",
      "19  The sub-prompt was tasked with identifying tab...   \n",
      "20  The generated response incorrectly identified ...   \n",
      "21                                                N/A   \n",
      "22                                                N/A   \n",
      "23  The sub-prompt was tasked with identifying all...   \n",
      "\n",
      "                                          suggestions  score  \n",
      "0                                                 N/A      5  \n",
      "1                                                 N/A      5  \n",
      "2                                                 N/A      5  \n",
      "3   The sub-prompt should be enhanced to better pa...      1  \n",
      "4   The sub-prompt should be refined to more accur...      2  \n",
      "5   The sub-prompt should be refined to more stric...      2  \n",
      "6                                                 N/A      5  \n",
      "7                                                 N/A      5  \n",
      "8                                                 N/A      5  \n",
      "9   The sub-prompt should be refined to more stric...      2  \n",
      "10                                                N/A      5  \n",
      "11                                                N/A      5  \n",
      "12  The sub-prompt should be refined to more stric...      2  \n",
      "13                                                N/A      5  \n",
      "14  Refine the prompt to emphasize that only table...      2  \n",
      "15                                                N/A      5  \n",
      "16                                                N/A      5  \n",
      "17  The sub-prompt should be more explicit in inst...      2  \n",
      "18                                                N/A      5  \n",
      "19  The sub-prompt should be refined to ensure it ...      2  \n",
      "20  Refine the prompt to emphasize identifying onl...      2  \n",
      "21                                                N/A      5  \n",
      "22                                                N/A      5  \n",
      "23  Refine the sub-prompt to explicitly instruct t...      1  \n",
      "\n",
      "\n",
      "---------------------- DONE ---------------\n"
     ]
    }
   ],
   "source": [
    "## TABLES PROMPT\n",
    "output_filename_table = \"./table_evaluations_v3.csv\"\n",
    "prompt_to_evaluate_table = \"table prompt result\"\n",
    "table_prompt_purpose = \"Identify a definitive list of tables and columns that will actually be used in the SELECT, FROM, and JOIN clauses. Inclusion of extraneous tables and columns does not make the response incorrect.\"\n",
    "original_prompt_table = \"\"\"Given the user question and the database schema context, identify the most relevant tables and columns needed to answer the question. \n",
    "    These tables and columns will be used in the SELECT, FROM and JOIN clauses.\n",
    "    Focus on tables and columns that directly relate to the entities and operations mentioned in the query. Consider table relationships and how they are used for joins or filtering.\n",
    "\n",
    "    Output your answer as a JSON object with 'tables' (list of table names) and 'columns' (list of 'table_name.column_name' strings) and 'reasoning' (string).\n",
    "\n",
    "    --- Schema Context ---\n",
    "    {schema_context}\n",
    "\n",
    "    --- User Question ---\n",
    "    {query_str}\n",
    "\n",
    "    JSON Output:\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "## GROUPING PROMPT\n",
    "output_filename_grouping = \"./grouping_evaluations_v2.csv\"\n",
    "prompt_to_evaluate_grouping = \"grouping prompt result including CTE\"\n",
    "grouping_prompt_purpose = \"To identify any GROUP BY clauses and the associated aggregate functions (COUNT, SUM, AVG, etc.).\"\n",
    "original_prompt_grouping = \"\"\"Given the user question, the previously identified tables and columns, and the business terms context, determine if any grouping or aggregation is required.\n",
    "    If so, list the columns to group by and briefly explain why. If not, state 'No grouping needed'.\n",
    "    Output as a JSON object with 'group_by_columns' (list of 'table_name.column_name' strings) and 'reasoning' (string).\n",
    "    \n",
    "    --- Schema Context ---\n",
    "    {schema_context}\n",
    "    --- Business Terms Context ---\n",
    "    {business_terms_context}\n",
    "    --- User Question ---\n",
    "    {query_str}\n",
    "    --- Identified Tables/Columns ---\n",
    "    {identified_tables_columns_json}\n",
    "    \n",
    "    JSON Out\n",
    "\"\"\"\n",
    "\n",
    "## Calculations prompt\n",
    "output_filename_calculations = \"./calculations_evaluations_v2.csv\"\n",
    "prompt_to_evaluate_calculations = \"calculations prompt result\"\n",
    "calculations_prompt_purpose = \"To define complex mathematical formulas or metrics that aren't simple aggregations (e.g., revenue / transactions).\"\n",
    "original_prompt_calculations = \"\"\"Given the user question, the previously identified tables and columns, and the business terms context, **determine only the core mathematical or aggregate calculations required to directly answer the user's question.**\n",
    "\n",
    "**Focus strictly on arithmetic operations (e.g., +, -, *, /) and aggregate functions (e.g., SUM, COUNT, AVG, MIN, MAX) that directly compute a value.**\n",
    "\n",
    "**DO NOT include:**\n",
    "* `GROUP BY` clauses\n",
    "* `ORDER BY` clauses\n",
    "* `HAVING` clauses\n",
    "* `WHERE` clauses (these are for filtering, a separate step)\n",
    "* Table joins or other structural SQL elements\n",
    "\n",
    "If no direct calculations are needed (e.g., the question is simply asking to retrieve raw data or identify entities without aggregation/arithmetic), state 'No calculations needed'.\n",
    "\n",
    "Output as a JSON object with 'calculations' (a list of concise 'calculation_string' strings) and 'reasoning' (a string explaining *why* each listed calculation is necessary to answer the question, without describing the full SQL logic).\n",
    "\n",
    "--- Schema Context ---\n",
    "{schema_context}\n",
    "--- Business Terms Context ---\n",
    "{business_terms_context}\n",
    "--- User Question ---\n",
    "{query_str}\n",
    "--- Identified Tables/Columns ---\n",
    "{identified_tables_columns_json}\n",
    "\n",
    "JSON Output:\n",
    "\"\"\"\n",
    "\n",
    "## Filtering prompt\n",
    "output_filename_filtering = \"./filtering_evaluations_v2.csv\"\n",
    "prompt_to_evaluate_filtering = \"filtering prompt result\"\n",
    "filtering_prompt_purpose = \"To identify conditions that filter the data. This step must distinguish between pre-aggregation filters (WHERE) and post-aggregation filters (HAVING). This prompt is not responsible for CTEs.\"\n",
    "original_prompt_filtering =   \"\"\"Given the user question, the previously identified tables and columns, and the business terms context, determine if any filtering is required.\n",
    "    If so, list the filtering. If not, state 'No filtering needed'.\n",
    "    Output as a JSON object with 'filters' (list of 'filter' strings) and 'reasoning' (string).\n",
    "    \n",
    "    --- Schema Context ---\n",
    "    {schema_context}\n",
    "    --- Business Terms Context ---\n",
    "    {business_terms_context}\n",
    "    --- User Question ---\n",
    "    {query_str}\n",
    "    --- Identified Tables/Columns ---\n",
    "    {identified_tables_columns_json}\n",
    "    \n",
    "    JSON Output:\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "###########################\n",
    "test_directory = \"../application/prompt_logs/\"\n",
    "gold_directory = \"../../spider/Spider2-main/spider2-lite/evaluation_suite/gold/sql/\"\n",
    "gold_tables_file = \"../../spider/Spider2-main/methods/gold-tables/spider2-lite-gold-tables.jsonl\"\n",
    "original_questions = \"../sql_test_set.jsonl\"\n",
    "\n",
    "print('\\n\\n ---- START TABLES ------------')\n",
    "eval_subprompts(test_directory=test_directory,\n",
    "                                gold_directory=gold_directory,\n",
    "                                output_filename=output_filename_table,\n",
    "                                prompt_to_evaluate = prompt_to_evaluate_table,\n",
    "                                gold_tables_file = gold_tables_file,\n",
    "                                orig_prompt =original_prompt_table,\n",
    "                                original_questions_file=original_questions, \n",
    "                                prompt_purpose=table_prompt_purpose)\n",
    "\n",
    "#print('\\n\\n ---- START GROUPING ------------')\n",
    "#eval_subprompts(test_directory=test_directory,\n",
    "#                                gold_directory=gold_directory,\n",
    "#                                output_filename=output_filename_grouping,\n",
    "#                                prompt_to_evaluate = prompt_to_evaluate_grouping,\n",
    "#                                gold_tables_file = gold_tables_file,\n",
    "#                                orig_prompt =original_prompt_table,\n",
    "#                                original_questions_file=original_questions, \n",
    "#                                prompt_purpose=grouping_prompt_purpose)\n",
    "\n",
    "#print('\\n\\n ---- START CALCULATIONS ------------')\n",
    "#eval_subprompts(test_directory=test_directory,\n",
    "#                                gold_directory=gold_directory,\n",
    "#                                output_filename=output_filename_calculations,\n",
    "#                                prompt_to_evaluate = prompt_to_evaluate_calculations,\n",
    "#                                gold_tables_file = gold_tables_file,\n",
    "#                                orig_prompt =original_prompt_table,\n",
    "#                                original_questions_file=original_questions, \n",
    "#                                prompt_purpose=calculations_prompt_purpose)\n",
    "\n",
    "#print('\\n\\n ---- START FILTERING ------------')\n",
    "#eval_subprompts(test_directory=test_directory,\n",
    "#                                gold_directory=gold_directory,\n",
    "#                                output_filename=output_filename_filtering,\n",
    "#                                prompt_to_evaluate = prompt_to_evaluate_filtering,\n",
    "#                                gold_tables_file = gold_tables_file,\n",
    "#                                orig_prompt =original_prompt_table,\n",
    "#                                original_questions_file=original_questions, \n",
    "#                                prompt_purpose=filtering_prompt_purpose)\n",
    "\n",
    "\n",
    "\n",
    "print('\\n\\n---------------------- DONE ---------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ec4a220-218f-437e-8979-4e6628179656",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eb634bd-6638-4d9b-a9e7-08e15a003a97",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
